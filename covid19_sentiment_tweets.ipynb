{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentiment140_covid19_tweets.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FT-RaDvoJhiF"},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6DrkFb61yYfT"},"source":["!pip install transformers\n","!pip install emoji\n","!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ob9DbZuaxXCW"},"source":["import transformers\n","from transformers import BertModel, BertTokenizer, BertConfig, AdamW, get_linear_schedule_with_warmup, pipeline, AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","import emoji\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from datasets import load_dataset\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import matplotlib\n","from matplotlib import rc\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","from textwrap import wrap\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import statistics\n","import re\n","import os\n","import random\n","from pprint import pprint\n","\n","# %reload_ext watermark\n","# %watermark -v -p numpy,pandas,torch,transformers\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilgCtr688kRy"},"source":["from tensorflow.python.client import device_lib\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","torch.cuda.get_device_name()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ipaFwRGMeOrr"},"source":["**Data Preprocessing**"]},{"cell_type":"code","metadata":{"id":"FSutsfOI8CFs"},"source":["# Preprocessing\n","\n","def text_preprocessing(s):\n","  # Remove url's\n","  s = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])', ' ',s)\n","\n","  # Change 't to 'not'\n","  s = re.sub(r\"\\'t\", \" not\", s)\n","  # Replace '&amp;' with '&'\n","  s = re.sub(r'&amp;', '&', s)\n","  # Remove retweet indicator RT\n","  s = re.sub(r'(RT?)[\\s]',' ',s)\n","  # Change emoji to ascii\n","  s = emoji.demojize(s, delimiters=(\"\", \"\"))\n","  # Remove \\n \n","  s = re.sub(r'\\n[\\s]',' ',s)\n","  # Remov apostrophe \n","  s = re.sub(r\"\\'\", '', s)\n","  # Remove punctuations \n","  s = re.sub(r'([\\'\\\"\\(\\)\\\\\\/\\#\\â\\€\\”\\@])', r'', s)\n","  s = re.sub(r'([\\¬\\‡\\ï\\¸\\\\¬\\‡\\ï])', r'', s)\n","\n","  # Remove trailing whitespace\n","  s = re.sub(r'\\s+', ' ', s).strip()\n","  return s"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xocdb-DjrlHL"},"source":["**Sentiment140 dataset for training**"]},{"cell_type":"code","metadata":{"id":"Xwm5uplcS5K9"},"source":["# from datasets import load_dataset\n","from pprint import pprint\n","dataset = load_dataset('sentiment140', split='train')\n","pprint(dataset[0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"paNLOQ0-pU-1"},"source":["#reduce dataset\n","df = pd.DataFrame(dataset) \n","sentiment140 = df.sample(n = 100000, random_state=25)\n","sentiment140.reset_index(drop=True,inplace=True)\n","sentiment140.loc[(sentiment140.sentiment == 4),'sentiment']= 1\n","# sentiment140.sentiment.unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzjLYKwva104"},"source":["# # Sentiment141 - add labelled COVID-19 tweets\n","# ## Combine reduced Sentiment140 with labelled covid tweets\n","# # print('sentiment140 ',sentiment140.info())\n","# labelled_covid = pd.read_csv(\"/content/labelled_tweets.csv\")\n","# # print('labelled_covid ',labelled_covid.info())\n","# sentiment141 = pd.merge(sentiment140, labelled_covid, how='outer') #left_on=['date'], right_on=['date'],how='outer')\n","# sentiment141.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kPr5oChN24of"},"source":["# how balanced is the reduced dataset\n","p = sentiment140.apply(lambda x: True if x['sentiment'] == 1 else False , axis=1)\n","n = sentiment140.apply(lambda x: True if x['sentiment'] == 0 else False , axis=1)\n","pRows = len(p[p == True].index)\n","nRows = len(n[n == True].index)\n","print('Positive: ', pRows)\n","print('Negative: ', nRows) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJwaVYg3y7sK"},"source":["# sentiment140 for clean runs\n","\n","clean_samples = pd.DataFrame()\n","clean_samples['label'] = sentiment140.sentiment #sentiment141.sentiment\n","for i in range(len(sentiment140)): #range(len(sentiment141))\n","  dirty_txt = sentiment140.at[i, 'text'] #sentiment141.at[i, 'text']\n","  # print(dirty_txt)\n","  clean_txt = text_preprocessing(dirty_txt)\n","  # print(clean_txt)\n","  clean_samples.at[i, 'text'] = clean_txt\n","# clean_samples.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4D8plHRbjoP"},"source":["# # Sentiment140/1 unclean runs\n","# sentiment140 = sentiment140.rename(columns = {'sentiment': 'label'}) #sentiment141 = sentiment141.rename(columns = {'sentiment': 'label'})\n","# clean_samples = sentiment140 #sentiment140"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LmGIREG2Qr_i"},"source":["#determine length of sequences and set a fixed length for sample tweets\n","\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')##### changed model\n","token_lens = []\n","for txt in clean_samples.text:\n","  tokens = tokenizer.encode(txt, max_length=512,truncation=True)\n","  token_lens.append(len(tokens))\n","\n","print(max(token_lens))\n","MAX_LEN = (max(token_lens))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ExuiuoP8ziny"},"source":["\n","\n","df_train, df_val = train_test_split(\n","  clean_samples,\n","  test_size=0.3,\n","  random_state=25\n",")\n","df_val, df_test = train_test_split(\n","  df_val,\n","  test_size=0.3,\n","  random_state=25\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I7Q-kRP-39c6"},"source":["print('train: ',df_train.shape)\n","print('validation: ',df_val.shape)\n","print('test: ',df_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"feQcgPu081MB"},"source":["class TwitterDataset(Dataset):\n","  \n","  def __init__(self, text, label, tokenizer, max_len):\n","    self.text = text\n","    self.label = label\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","  \n","  def __len__(self):\n","    return len(self.text)\n","  \n","  def __getitem__(self, item):\n","    text = str(self.text[item])\n","    label = self.label[item]\n","    \n","    encoding = self.tokenizer.encode_plus(\n","      text,\n","      add_special_tokens=True,\n","      max_length=self.max_len,\n","      return_token_type_ids=False,\n","      padding='max_length',\n","      # pad_to_max_length=True,\n","      # padding=True,\n","      truncation=True,\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","    )\n","    return {\n","      'text': text,\n","      'input_ids': encoding['input_ids'].flatten(),\n","      'attention_mask': encoding['attention_mask'].flatten(),\n","      'label': torch.tensor(label, dtype=torch.long)\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1YhyLYXjJou"},"source":["# Create a data loader\n","\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size):\n","  ds = TwitterDataset(\n","    text=df.text.to_numpy(),\n","    label=df.label.to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len\n","    # padding=True\n","  )\n","  return DataLoader(\n","    ds,\n","    batch_size=batch_size,\n","    num_workers=4\n","  )\n","\n","BATCH_SIZE = 16\n","train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN,  BATCH_SIZE)\n","val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN ,BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OR1wHKIcjoWO"},"source":["data = next(iter(train_data_loader))\n","data.keys()\n","\n","print(data['input_ids'].shape)\n","print(data['attention_mask'].shape)\n","print(data['label'].shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VXTMO9-YmFAB"},"source":["class SentimentClassifier(nn.Module):\n","  def __init__(self, n_classes):\n","    super(SentimentClassifier, self).__init__()\n","    self.bert = BertModel.from_pretrained('bert-base-cased')\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n","    \n","  def forward(self, input_ids, attention_mask):\n","    _, pooled_output = self.bert(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )\n","    output = self.drop(pooled_output)\n","    return self.out(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iw_IXCeTmjp7"},"source":["\n","class_names = ['0', '1'] #['Negative', 'Positive'] #['0', '4'] #\n","\n","model = SentimentClassifier(len(class_names))\n","model = model.to(device)\n","\n","input_ids = data['input_ids'].to(device)\n","attention_mask = data['attention_mask'].to(device)\n","print(input_ids.shape) # batch size x seq length\n","print(attention_mask.shape) # batch size x seq length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FgyR3leX2p0x"},"source":["\n","m = nn.Softmax(dim=1)\n","input = model(input_ids, attention_mask)\n","output = m(input)\n","output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Lsg2ofGskdo"},"source":["**Training**"]},{"cell_type":"code","metadata":{"id":"mHNn1HhtRADO"},"source":["EPOCHS = 2\n","# optimizer = AdamW(model.parameters(), lr=3e-5, correct_bias=False)\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9XHB3-6RK-0"},"source":["def train_epoch(\n","  model,\n","  data_loader,\n","  loss_fn,\n","  optimizer,\n","  device,\n","  scheduler,\n","  n_examples\n","):\n","  model = model.train()\n","  losses = []\n","  correct_predictions = 0\n","  for d in data_loader:\n","    input_ids = d[\"input_ids\"].to(device)\n","    attention_mask = d[\"attention_mask\"].to(device)\n","    label = d[\"label\"].to(device)\n","    outputs = model(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, label)\n","    correct_predictions += torch.sum(preds == label)\n","    losses.append(loss.item())\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","  return correct_predictions.double() / n_examples, np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRJ2319lsUho"},"source":["def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","  losses = []\n","  correct_predictions = 0\n","  with torch.no_grad():\n","    for d in data_loader:\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      label = d[\"label\"].to(device)\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","      loss = loss_fn(outputs, label)\n","      correct_predictions += torch.sum(preds == label)\n","      losses.append(loss.item())\n","  return correct_predictions.double() / n_examples, np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFbPHHyMRfrc"},"source":["%%time\n","history = defaultdict(list)\n","best_accuracy = 0\n","for epoch in range(EPOCHS):\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 10)\n","  train_acc, train_loss = train_epoch(\n","    model,\n","    train_data_loader,\n","    loss_fn,\n","    optimizer,\n","    device,\n","    scheduler,\n","    len(df_train)\n","  )\n","  print(f'Train loss {train_loss} accuracy {train_acc}')\n","  val_acc, val_loss = eval_model(\n","    model,\n","    val_data_loader,\n","    loss_fn,\n","    device,\n","    len(df_val)\n","  )\n","  print(f'Val   loss {val_loss} accuracy {val_acc}')\n","  print()\n","  history['train_acc'].append(train_acc)\n","  history['train_loss'].append(train_loss)\n","  history['val_acc'].append(val_acc)\n","  history['val_loss'].append(val_loss)\n","  if val_acc > best_accuracy:\n","    torch.save(model.state_dict(), 'best_model_state.bin')\n","    best_accuracy = val_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lD_CJFuM7FRM"},"source":["plt.plot(history['train_acc'], label='train accuracy')\n","plt.plot(history['val_acc'], label='validation accuracy')\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0.6, 1])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WqBexTQ5F8sx"},"source":["**Evaluation**"]},{"cell_type":"code","metadata":{"id":"XqlKy4DHzzGZ"},"source":["model = SentimentClassifier(len(class_names))\n","model.load_state_dict(torch.load('best_model_state.bin'))\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3re5crfF8YV"},"source":["test_acc, _ = eval_model(\n","  model,\n","  test_data_loader,\n","  loss_fn,\n","  device,\n","  len(df_test)\n",")\n","test_acc.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y730gt8EJPK6"},"source":["def get_predictions(model, data_loader):\n","  model = model.eval()\n","  texts = []\n","  predictions = []\n","  prediction_probs = []\n","  real_values = []\n","  with torch.no_grad():\n","    for d in data_loader:\n","      texts = d[\"text\"]\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      label = d[\"label\"].to(device)\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","      texts.extend(texts)\n","      predictions.extend(preds)\n","      prediction_probs.extend(outputs)\n","      real_values.extend(label)\n","  predictions = torch.stack(predictions).cpu()\n","  prediction_probs = torch.stack(prediction_probs).cpu()\n","  real_values = torch.stack(real_values).cpu()\n","  return texts, predictions, prediction_probs, real_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"umcYQL-UJllh"},"source":["y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n","  model,\n","  test_data_loader\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-3PT9QDcJrrY"},"source":["print(classification_report(y_test, y_pred, target_names=class_names))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjfWCgZIKY0x"},"source":["def show_confusion_matrix(confusion_matrix):\n","  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n","  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n","  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n","  plt.ylabel('True sentiment')\n","  plt.xlabel('Predicted sentiment');\n","cm = confusion_matrix(y_test, y_pred)\n","df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n","show_confusion_matrix(df_cm)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5DvanJyt5xX7"},"source":["**COVID Tweets**"]},{"cell_type":"code","metadata":{"id":"3vqf_ZcaGr_S"},"source":["# Merge all tweet csv files to one\n","# import os\n","# import glob\n","# import pandas as pd\n","# os.chdir(\"/content/drive/My Drive/Dissertation/tweets/\")\n","# extension = 'csv'\n","# all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n","# #combine all files in the list\n","# combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n","# #export to csv\n","# combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')\n","# combined_csv.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4p8ve5TDIOC"},"source":["# read in all covid tweets\n","tweets = pd.read_csv(\"/content/combined_csv.csv\") \n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', None)\n","print(tweets.tweet_text.tail())\n","tweets.info()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"juYkurjvup8X"},"source":["# clean up covid tweets for 'clean' runs\n","\n","for i in range(len(tweets)):\n","  dirty_txt = tweets.at[i, 'full_text']\n","  clean_txt = text_preprocessing(dirty_txt)\n","  tweets.at[i, 'clean_text'] = clean_txt\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', None)\n","tweets.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQx-FfnccxDO"},"source":["# # for unclean run rename full_text to clean_text\n","# tweets = tweets.rename(columns = {'full_text': 'clean_text'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3lEsf5B07w-u"},"source":["#determine length of sequences and set a fixed length\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased') \n","token_lens = []\n","for txt in tweets.clean_text:\n","  tokens = tokenizer.encode(txt, max_length=512,truncation=True)\n","  token_lens.append(len(tokens))\n","\n","print(max(token_lens))\n","MAX_LEN = (max(token_lens))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsCacqodTz3m"},"source":["# Predicting COVID tweets dataset\n","\n","pred_tweets = tweets\n","\n","for i in range(len(pred_tweets)):\n","  encoded_test = tokenizer.encode_plus(\n","    pred_tweets.at[i,'clean_text'],\n","    max_length=MAX_LEN,\n","    add_special_tokens=True,\n","    return_token_type_ids=False,\n","    padding='max_length',\n","    truncation=True,\n","    return_attention_mask=True,\n","    return_tensors='pt'\n","  )\n","  input_ids = encoded_test['input_ids'].to(device)\n","  attention_mask = encoded_test['attention_mask'].to(device)\n","  label = torch.tensor([1]) #.unsqueeze(0)\n","  output = model(input_ids, attention_mask)\n","  _, pred_labels = torch.max(output, dim=1)\n","  sm = torch.nn.Softmax(dim=1)\n","  probabilities = sm(output) \n","  prob = probabilities[0]\n","  prob = prob.detach().cpu().numpy()\n","  probs = prob.max()\n","  pred_tweets.at[i,'pred_labels'] = class_names[pred_labels]\n","  pred_tweets.at[i,'probability'] = probs\n","\n","print(pred_tweets.head())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qz-5rhtVWO6n"},"source":["# Plotting Positive and Negative tweet probabilities\n","neg = list(pred_tweets[pred_tweets['pred_labels'] =='0']['probability'])\n","pos = list(pred_tweets[pred_tweets['pred_labels'] =='1']['probability'])\n","labels_names = ['0','1']\n","plt.hist([neg, pos],label = labels_names)\n","# Plot formatting\n","plt.legend()\n","plt.xlabel('Prediction Probability')\n","plt.ylabel('Count of Tweets')\n","plt.title('Histogram of probabilities of positive and negative tweets')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FlVrdbv6bAbw"},"source":["to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n","pred_tweets.to_csv(path_or_buf = r'pred_tweets.csv', index=False)\n","# pred_tweets.to_csv(path_or_buf = r\"/content/combined_preds_\" + to_csv_timestamp +'.csv', index=False) # make sure drive is mounted!!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMZy2_JcpDAy"},"source":["# Numbers of positive and negative predictions\n","p = pred_tweets.apply(lambda x: True if x['pred_labels'] == '1' else False , axis=1)\n","n = pred_tweets.apply(lambda x: True if x['pred_labels'] == '0' else False , axis=1)\n","pRows = len(p[p == True].index)\n","nRows = len(n[n == True].index)\n","print('Number tweets predicted as positive: ', pRows)\n","print('Number tweets predicted as negative: ', nRows)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lkVZBvULwXGf"},"source":["print(pred_tweets.info())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I3ZcU76VJI68"},"source":["# pred_tweets = pd.read_csv(\"/content/combined_preds.csv\")\n","pred_cases = pd.read_csv(\"/content/data_2020-Nov-04.csv\")\n","\n","pred_cases.drop(columns=['areaType','areaName','areaCode','cumCasesBySpecimenDate'],inplace=True)\n","pred_cases = pred_cases.rename(columns = {'newCasesBySpecimenDate': 'CasesPerDay'})\n","pred_cases = pred_cases[pred_cases.date > '2020-10-15']\n","pred_cases = pred_cases[pred_cases.date != '2020-11-03']\n","pred_cases = pred_cases.sort_values(by='date').reset_index(drop=True)\n","print(pred_cases)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_VVuuMubkB8Z"},"source":["# # print(pred_cases.info())\n","# # pred_cases.drop(columns=['areaType','areaName','areaCode','cumCasesBySpecimenDate'],inplace=True)\n","# # pred_cases.date  # pd.to_datetime\n","# # pred_cases['date'] = pd.to_datetime(pred_cases['date']) \n","# # date_obj = datetime.strptime('2020-10-15', '%Y-%m-%d')\n","# # print(type(date_obj), pred_cases.dtypes)\n","# # mydate = pd.to_datetime('2020-10-15', format='%Y-%m-%d')\n","# # print(mydate)\n","# pred_cases = pred_cases[pred_cases.date > '2020-10-15']\n","# # print(pred_cases)\n","# pred_cases = pred_cases.sort_values(by='date').reset_index(drop=True)\n","# print(pred_cases)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WA7rwoZqDaRS"},"source":["# summarise total number of positive and negative predictions per day and combine with cases\n","\n","pred_tweets['created_at'] = pd.to_datetime(pred_tweets['created_at'])\n","pred_tweets['date'] = pred_tweets['created_at'].dt.strftime('%Y-%m-%d')\n","\n","pred_summary = pred_tweets[['date','pred_labels','created_at']].groupby(['date','pred_labels'],as_index=False).count()\n","\n","for i in range(len(pred_summary)):\n","  pred_date = pred_summary.at[i,'date']\n","  pred_label = pred_summary.at[i,'pred_labels']\n","  pred_obj = pred_summary[(pred_summary['date']==pred_date)] # & (pred_summary['pred_labels']==pred_label)]\n","  print('pred_obj    ',pred_obj)\n","  pred_index = pred_cases[pred_cases['date']==pred_date].index[0]\n","  if pred_obj.at[i, 'pred_labels']==1:\n","    pred_cases.at[pred_index, 'positive_tweets']=pred_obj.at[i,'created_at']\n","  elif (pred_obj.at[i, 'pred_labels']==0):\n","    pred_cases.at[pred_index, 'negative_tweets']=pred_obj.at[i, 'created_at']\n","    # print('********elif*******',pred_cases.at[pred_index])\n","\n","print(pred_cases)\n","\n","# print(pred_figures.info())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxSZAMbaakhG"},"source":["# Plot summary tweets/cases\n","\n","pred_cases.plot(x='date', y=['CasesPerDay', 'negative_tweets','positive_tweets'])\n"],"execution_count":null,"outputs":[]}]}